{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Getting the MNIST data and storing it in ./MNIST_data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32 # Number of images to run at each batch\n",
    "learning_rate = 0.0001 # Learning rate for optimizer\n",
    "logdir = './logs/dcgan_mnist/' # Logdir for tensorboard summaries\n",
    "num_adversarial_iter = 5000000 # Number of batches for the adversarial training\n",
    "num_discriminator_iter = 10000 # Number of batches to pretrain the discriminator\n",
    "pkeep = 0.75 # Probability with which to keep nodes during dropout\n",
    "print_all = False # Print all tensor names and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to create convolutional and fully connected layers with tensorboard summaries\n",
    "\n",
    "def conv_layer(input, kernel_size, channels_in, channels_out, stride, name, reuse_variables=None):\n",
    "    with tf.variable_scope(name, reuse=reuse_variables):\n",
    "        # Defining the graph of the operation\n",
    "        w = tf.get_variable('weights', [kernel_size, kernel_size, channels_in, channels_out],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b = tf.get_variable('biases', [channels_out], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        act = tf.nn.elu(conv + b, name='activation')\n",
    "\n",
    "        # Dropout regularization to prevent overfit\n",
    "        act_dropout = tf.nn.dropout(act, pkeep)\n",
    "\n",
    "        # Creating summaries for Tensorboard\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "\n",
    "        # Printing operation names and shapes for debugging\n",
    "        global print_all\n",
    "        if print_all:\n",
    "            print(w.name, w.shape)\n",
    "            print(b.name, b.shape)\n",
    "            print(conv.name, conv.shape)\n",
    "            print(act.name, act.shape)\n",
    "\n",
    "        return act_dropout\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name, activation, reuse_variables=None):\n",
    "    with tf.variable_scope(name, reuse=reuse_variables):\n",
    "        # Defining the graph of the operation\n",
    "        w = tf.get_variable('weights', [size_in, size_out], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b = tf.get_variable('biases', [size_out], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        logit = tf.matmul(input, w) + b\n",
    "\n",
    "        # Creating summaries for Tensorboard\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "\n",
    "        # Printing operation names and shapes for debugging\n",
    "        global print_all\n",
    "        if print_all:\n",
    "            print(w.name, w.shape)\n",
    "            print(b.name, b.shape)\n",
    "            print(logit.name, logit.shape)\n",
    "\n",
    "        # Covering the different cases needed\n",
    "        if activation == 'linear':\n",
    "            logit_dropout = tf.nn.dropout(logit, pkeep)\n",
    "            tf.summary.histogram(\"activations\", logit)\n",
    "            return logit_dropout\n",
    "        elif activation == 'elu':\n",
    "            act = tf.nn.elu(logit, name='activation')\n",
    "            act_dropout = tf.nn.dropout(act, pkeep)\n",
    "            tf.summary.histogram(\"activations\", act)\n",
    "            if print_all:\n",
    "                   print(act.name, act.shape)\n",
    "            return act_dropout\n",
    "\n",
    "def deconv_layer(input, kernel_size, channels_in, channels_out, stride, name, activation, reuse_variables=None):\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    with tf.variable_scope(name, reuse=reuse_variables):\n",
    "        # Defining the graph of the operation\n",
    "        w = tf.get_variable('weights', [kernel_size, kernel_size, channels_out, channels_in],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b = tf.get_variable('biases', [channels_out], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        conv = tf.nn.conv2d_transpose(input, w,\n",
    "                                      output_shape=[input_shape[0], input_shape[1]*stride, input_shape[2]*stride, channels_out],\n",
    "                                      strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "\n",
    "        # Creating summaries for Tensorboard\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "\n",
    "        # Printing operation names and shapes for debugging\n",
    "        global print_all\n",
    "        if print_all:\n",
    "            print(w.name, w.shape)\n",
    "            print(b.name, b.shape)\n",
    "            print(conv.name, conv.shape)\n",
    "\n",
    "        # Covering the different cases needed\n",
    "        if activation == 'elu':\n",
    "            act = tf.nn.elu(conv + b, name='activation')\n",
    "            act_dropout = tf.nn.dropout(act, pkeep)\n",
    "            if print_all:\n",
    "                print(act.name, act.shape)\n",
    "            tf.summary.histogram(\"activations\", act)\n",
    "            return act_dropout\n",
    "        elif activation == 'sigmoid':\n",
    "            act = tf.nn.sigmoid(conv + b, name='activation')\n",
    "            if print_all:\n",
    "                print(act.name, act.shape)\n",
    "            tf.summary.histogram(\"activations\", act)\n",
    "            return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "\n",
    "# input: [batch_size, 28, 28, 1]\n",
    "# output: [batch_size, 1]\n",
    "\n",
    "# 2 convolutional layers: input_shape = [-1, 28, 28, 1], output_shape = [-1, 7, 7, 64]\n",
    "# - Conv1: 32 5x5 filters, stride 2x2, padding = same, input_shape = [-1, 28, 28, 1]\n",
    "#   - weights: shape = [5, 5, 1, 32], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [32], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - strides: [1, 2, 2, 1]\n",
    "#   - activation: tf.nn.elu\n",
    "# - Conv2: 64 5x5 filters, stride 2x2, padding = same, input_shape = [-1, 14, 14, 64]\n",
    "#   - weights: shape = [5, 5, 32, 64], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [64], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - strides: [1, 2, 2, 1]\n",
    "#   - activation: tf.nn.elu\n",
    "# Reshape into vectors: input_shape = [-1, 7, 7, 64], output_shape = [-1, 7 * 7 * 64]\n",
    "# 3 fully connected layers: input_shape = [-1, 3136], output_shape = [-1, 1]\n",
    "# - FC1: input_shape = [-1, 3136]\n",
    "#   - weights: shape = [3136, 1024], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [1024], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - activation: tf.nn.elu\n",
    "# - FC2: input_shape = [-1, 1024]\n",
    "#   - weights: shape = [1024, 128], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [128], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - activation: tf.nn.elu\n",
    "# - FC3: input_shape = [-1, 128]\n",
    "#   - weights: shape = [128, 1], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [1], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - activation: linear\n",
    "\n",
    "# Loss: tf.sigmoid_cross_entropy_with_logits\n",
    "# Optimizer: tf.train.AdamOptimizer\n",
    "\n",
    "def discriminator(images, reuse_variables=None):\n",
    "    if not reuse_variables:\n",
    "        print('Initializing Discriminator')\n",
    "    else:\n",
    "        print('Reusing Discriminator')\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n",
    "        d_conv_1 = conv_layer(input=images, kernel_size=5, channels_in=1, channels_out= 32, stride=2,\n",
    "                                    name='discriminator_conv_1', reuse_variables=reuse_variables)\n",
    "        d_conv_2 = conv_layer(input=d_conv_1, kernel_size=5, channels_in=32, channels_out= 64, stride=2,\n",
    "                                    name='discriminator_conv_2', reuse_variables=reuse_variables)\n",
    "        flattened = tf.reshape(d_conv_2, [-1, 7*7*64])\n",
    "        d_fc_1 = fc_layer(input=flattened, size_in=7*7*64, size_out=1024, activation='elu', name='discriminator_fc_1',\n",
    "                          reuse_variables=reuse_variables)\n",
    "        d_fc_2 = fc_layer(input=d_fc_1, size_in=1024, size_out=128, activation='elu', name='discriminator_fc_2',\n",
    "                          reuse_variables=reuse_variables)\n",
    "        d_fc_3 = fc_layer(input=d_fc_2, size_in=128, size_out=1, activation='linear', name='discriminator_fc_3',\n",
    "                          reuse_variables=reuse_variables)\n",
    "        return d_fc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the generator\n",
    "\n",
    "# input: [batch_size, 128]\n",
    "# output: [batch_size, 28, 28, 1]\n",
    "\n",
    "# 2 fully connected layers: input_shape = [-1, 128], output_shape = [-1, 7 * 7 * 64]\n",
    "# - FC1: input_shape = [-1, 128], output_shape = [-1, 1024]\n",
    "#   - weights: shape = [128, 1024], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [1024], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - activation: tf.nn.elu\n",
    "# - FC2: input_shape = [-1, 1024], output_shape = [-1, 7 * 7 * 128]\n",
    "#   - weights: shape = [1024, 7 * 7 * 64], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [7 * 7 * 64], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - activation: tf.nn.elu\n",
    "# Reshape data: input_shape = [-1,7*7*64] output_shape = [-1, 7, 7, 64]\n",
    "# 2 transpose convolution layers: input_shape = [-1, 7, 7, 64], output_shape = [-1, 28, 28, 1]\n",
    "# - Transpose_Conv1: input_shape = [-1, 7, 7, 64], output_shape = [-1, 14, 14, 32]\n",
    "#   - weights: shape = [5, 5, 64, 32], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [64], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - strides: [1, 2, 2, 1]\n",
    "#   - activation: tf.nn.elu\n",
    "# - Transpose_Conv2: input_shape = [-1, 14, 14, 32], output_shape = [-1, 28, 28, 1]\n",
    "#   - weights: shape = [5, 5, 32, 1], initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "#   - bias: shape = [1], initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "#   - strides: [1, 2, 2, 1]\n",
    "#   - activation: tf.nn.sigmoid\n",
    "\n",
    "# Loss: tf.sigmoid_cross_entropy_with_logits\n",
    "# Optimizer: tf.train.AdamOptimizer\n",
    "\n",
    "def generator(z, reuse_variables=None):\n",
    "    if not reuse_variables:\n",
    "        print('Initializing Generator')\n",
    "    else:\n",
    "        print('Reusing Generator')\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        g_fc_1 = fc_layer(input=z, size_in=128, size_out=1024, activation='elu', name='generator_fc_1', \n",
    "                          reuse_variables=reuse_variables)\n",
    "        g_fc_2 = fc_layer(input=g_fc_1, size_in=1024, size_out=7*7*64, activation='elu', name='generator_fc_2', \n",
    "                          reuse_variables=reuse_variables)\n",
    "        reshaped = tf.reshape(g_fc_2, [-1, 7, 7, 64])\n",
    "        g_deconv_1 = deconv_layer(input=reshaped, kernel_size=5, channels_in=64, channels_out= 32, stride=2,\n",
    "                                        name='generator_deconv_1', activation='elu', reuse_variables=reuse_variables)\n",
    "        g_deconv_2 = deconv_layer(input=g_deconv_1, kernel_size=5, channels_in=32, channels_out= 1, stride=2,\n",
    "                                        name='generator_deconv_2', activation='sigmoid', reuse_variables=reuse_variables)\n",
    "        return g_deconv_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing that the graphs work as intended\n",
    "# Using a constant zero vector of the correct shape to get the resulting tensors and their shapes\n",
    "\n",
    "#with print_all = True:\n",
    "#    const = tf.constant(0, shape=[28*28, 128], dtype=tf.float32)\n",
    "#    gen = generator(const)\n",
    "#    reshaped_const = tf.reshape(const, [-1, 28, 28, 1])\n",
    "#    discrim1 = discriminator(reshaped_const)\n",
    "#    discrim2 = discriminator(reshaped_const, True)\n",
    "\n",
    "\n",
    "# Print the nodes in the graph\n",
    "#[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Generator\n",
      "Initializing Discriminator\n",
      "Reusing Discriminator\n",
      "Reusing Generator\n"
     ]
    }
   ],
   "source": [
    "# Defining the graphs, loss, optimizers, and summaries\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Creating placeholders for input to graph\n",
    "z_placeholder = tf.placeholder(tf.float32, [batch_size, 128], name = 'z_placeholder')\n",
    "images_placeholder = tf.placeholder(tf.float32, [batch_size, 784], name = 'images_placeholder')\n",
    "images_reshaped = tf.reshape(images_placeholder, [-1, 28, 28, 1])\n",
    "tf.summary.image('Input_images', images_reshaped, 6)\n",
    "\n",
    "# Initializing Generator and Discriminators\n",
    "Generator = generator(z_placeholder)\n",
    "Discriminator_real = discriminator(images_reshaped)\n",
    "Discriminator_fake = discriminator(Generator, reuse_variables=True)\n",
    "\n",
    "# Defining loss functions for networks\n",
    "with tf.name_scope('Cross_Entropy'):\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Discriminator_real,\n",
    "                                                                         labels=tf.zeros_like(Discriminator_real)))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Discriminator_fake,\n",
    "                                                                         labels=tf.ones_like(Discriminator_fake)))\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Discriminator_fake,\n",
    "                                                                    labels=tf.zeros_like(Discriminator_fake)))\n",
    "    tf.summary.scalar('D_loss_real', D_loss_real)\n",
    "    tf.summary.scalar('D_loss_fake', D_loss_fake)\n",
    "    tf.summary.scalar('G_loss', G_loss)\n",
    "\n",
    "# Getting the variables for each network to update only certain weights during each optimization\n",
    "tvars = tf.trainable_variables()\n",
    "D_vars = [var for var in tvars if 'discriminator_' in var.name]\n",
    "G_vars = [var for var in tvars if 'generator_' in var.name]\n",
    "\n",
    "# Defining the optimizer (Adam adaptive learning rate and momemtum optimizer)\n",
    "with tf.name_scope('Train'):\n",
    "    D_train_real = tf.train.AdamOptimizer(learning_rate).minimize(D_loss_real, var_list=D_vars)\n",
    "    D_train_fake = tf.train.AdamOptimizer(learning_rate).minimize(D_loss_fake, var_list=D_vars)\n",
    "    G_train = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "# Defining graph for generating a sample image\n",
    "z_sample = tf.placeholder(dtype = tf.float32, shape = [6, 128], name='z_sample')\n",
    "sample_images = generator(z_sample, True)\n",
    "tf.summary.image('Generated_images', sample_images, 6)\n",
    "\n",
    "# Write out images to disk\n",
    "output_image_placeholder = tf.placeholder(dtype = tf.float32, shape = [28, 28, 1], name='output_image_placeholder')\n",
    "cast_image = tf.cast(tf.multiply(output_image_placeholder, 255), tf.uint8)\n",
    "encode_image = tf.image.encode_jpeg(cast_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAACaCAYAAADYUbuPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxv8DjYwq0GU/7usAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x252757c13c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Tensorflow session and intialize variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Summary merging and writing\n",
    "saver = tf.train.Saver()\n",
    "summ = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logdir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "writer.add_graph(sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Generate a sample image before training\n",
    "image = sess.run(sample_images, feed_dict={z_sample: np.random.normal(-1,1,[6,128])})\n",
    "fig, ax = plt.subplots(1, 6, figsize = (15, 24))\n",
    "for i, a in enumerate(ax):\n",
    "    a.imshow(image[i].squeeze(), cmap='gray_r') # Squeeze the output of the generator down to two dimensions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.697091 0.681913\n",
      "100 0.217473 0.151852\n",
      "200 0.195198 0.195068\n",
      "300 0.216743 0.216653\n",
      "400 0.108458 0.173323\n",
      "500 0.151738 0.259953\n",
      "600 0.15167 0.0866622\n",
      "700 0.151672 0.129976\n",
      "800 0.108323 0.129976\n",
      "900 0.129983 0.108313\n",
      "1000 0.151648 0.194954\n",
      "1100 0.151642 0.259935\n",
      "1200 0.108318 0.15163\n",
      "1300 0.0866538 0.15163\n",
      "1400 0.238281 0.19495\n",
      "1500 0.108311 0.129969\n",
      "1600 0.259939 0.21661\n",
      "1700 0.194952 0.129968\n",
      "1800 0.259935 0.216611\n",
      "1900 0.15165 0.151628\n",
      "2000 0.129968 0.194949\n",
      "2100 0.15163 0.129967\n",
      "2200 0.173297 0.216609\n",
      "2300 0.238271 0.23827\n",
      "2400 0.216611 0.108306\n",
      "2500 0.108306 0.0866446\n",
      "2600 0.21661 0.129966\n",
      "2700 0.064985 0.129966\n",
      "2800 0.19495 0.259931\n",
      "2900 0.23827 0.173287\n",
      "3000 0.0866452 0.0866446\n",
      "3100 0.21661 0.151626\n",
      "3200 0.173288 0.151627\n",
      "3300 0.173291 0.0866438\n",
      "3400 0.23827 0.108305\n",
      "3500 0.216609 0.25993\n",
      "3600 0.173287 0.259931\n",
      "3700 0.151627 0.173287\n",
      "3800 0.151628 0.303252\n",
      "3900 0.173288 0.108305\n",
      "4000 0.151628 0.194948\n",
      "4100 0.129965 0.216609\n",
      "4200 0.173287 0.194948\n",
      "4300 0.108305 0.0866435\n",
      "4400 0.108305 0.151626\n",
      "4500 0.194948 0.151626\n",
      "4600 0.23827 0.216609\n",
      "4700 0.194948 0.0866436\n",
      "4800 0.151626 0.173287\n",
      "4900 0.108305 0.173287\n",
      "5000 0.173287 0.151626\n",
      "5100 0.194948 0.25993\n",
      "5200 0.129965 0.281591\n",
      "5300 0.151626 0.129965\n",
      "5400 0.151626 0.108304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6be0820d4006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     sess.run([D_train_real, D_train_fake], feed_dict={images_placeholder: image_batch[0],\n\u001b[1;32m----> 7\u001b[1;33m                                                       z_placeholder: z_input})\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mimage_batch_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pretraining for discriminator\n",
    "for i in range(num_discriminator_iter):\n",
    "    image_batch = mnist.train.next_batch(batch_size)\n",
    "    z_input = np.random.normal(-1, 1, [batch_size, 128])\n",
    "\n",
    "    sess.run([D_train_real, D_train_fake], feed_dict={images_placeholder: image_batch[0],\n",
    "                                                      z_placeholder: z_input})\n",
    "    if i % 100 == 0:\n",
    "        image_batch_val = mnist.train.next_batch(batch_size)\n",
    "        z_val = np.random.normal(-1, 1, [batch_size, 128])\n",
    "\n",
    "        real_loss, fake_loss = sess.run([D_loss_real, D_loss_fake], feed_dict={images_placeholder: image_batch_val[0],\n",
    "                                                                                z_placeholder: z_val})\n",
    "        print(i, real_loss, fake_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train networks together\n",
    "plt.ion()\n",
    "\n",
    "for i in range(num_adversarial_iter):\n",
    "    image_batch = mnist.train.next_batch(batch_size)\n",
    "    z_input = np.random.normal(-1, 1, [batch_size, 128])\n",
    "\n",
    "    sess.run([D_train_real, D_train_fake, G_train], feed_dict={images_placeholder: image_batch[0],\n",
    "                                                               z_placeholder: z_input})\n",
    "\n",
    "    if i%100 == 0:\n",
    "        z_val = np.random.normal(-1, 1, [batch_size, 128])\n",
    "        image_batch_val = mnist.train.next_batch(batch_size)\n",
    "        z_summary = np.random.normal(-1, 1, [6, 128])\n",
    "        s = sess.run(summ, {z_placeholder: z_val, images_placeholder: image_batch_val[0], z_sample: z_summary})\n",
    "        writer.add_summary(s, i)\n",
    "        if i%10000 == 0:\n",
    "            image = sess.run(sample_images, feed_dict={z_sample: z_summary})\n",
    "            plt.close()\n",
    "            fig, ax = plt.subplots(1, 6, figsize = (15, 24))\n",
    "            for j, a in enumerate(ax):\n",
    "                im = image[j].squeeze()\n",
    "                a.imshow(im, cmap='gray_r') # Squeeze the output of the generator down to two dimensions\n",
    "            plt.show(block=False)\n",
    "            if i%100000 == 0:\n",
    "                #if not os.path.exists('sample_images'):\n",
    "                #    os.makedirs('sample_images')\n",
    "                #for j in range(image.shape[0]):\n",
    "                #    image_jpeg = sess.run(encode_image, feed_dict={output_image_placeholder: image[j] })\n",
    "                #    with open('sample_images/iter_{}_sample_{}.jpeg'.format(i,j), 'wb') as fd:\n",
    "                #        fd.write(image_jpeg)\n",
    "                if not os.path.exists('sample_generated'):\n",
    "                    os.makedirs('sample_generated')\n",
    "                for j in range(image.shape[0]):\n",
    "                    np.savetxt('sample_generated/iter_{}_sample_{}.csv'.format(i,j), image[j], delimiter=',')\n",
    "    if i%1000 == 0:\n",
    "        z_test = np.random.normal(-1, 1, [batch_size, 128])\n",
    "        image_batch_test = mnist.train.next_batch(batch_size)\n",
    "        d_loss_real, d_loss_fake, g_loss = sess.run([D_loss_real, D_loss_fake, G_loss],\n",
    "                                                    feed_dict={images_placeholder: image_batch_test[0],\n",
    "                                                               z_placeholder: z_test})\n",
    "        print(i, d_loss_real, d_loss_fake, g_loss)\n",
    "    if i%100000 == 0:\n",
    "        if not os.path.exists('saved_models'):\n",
    "            os.makedirs('saved_models')\n",
    "        print('Saving model at iteration {}'.format(i))\n",
    "        save_path = saver.save(sess, 'saved_models/model'.format(i), global_step=i)\n",
    "        print('Model saved at {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "## https://github.com/jonbruner/generative-adversarial-networks\n",
    "## https://github.com/soumith/ganhacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
